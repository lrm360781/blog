---
title: 如何优化数据导入
date: 2019-09-012 21:39:21
tags: MySQL
---
## 前言
我们有时会遇到批量数据导入的场景，而数据量稍微大点，会发现导入非常耗时间。这篇博客将介绍一些常用的加快数据导入的方法。

## 一次插入多行的值
插入行所需的时间由以下因素决定（参考MySQL 5.7参考手册：[8.2.4.1优化INSERT语句](https://dev.mysql.com/doc/refman/5.7/en/insert-optimization.html)）

* 连接：30%
* 向服务器发送查询：20%
* 解析查询：20%
* 插入行：10% * 行的大小
* 插入索引：10% * 索引数
* 结束：10%

可发现大部分时间耗费在客户端与服务端通信的时间，因此可以使用 insert 包含多个值来减少客户端和服务器之间的通信。

我们通过实验来验证下一次插入多行与一次插入一行的效率对比。

### 环境准备

```sql
use muke;                 /* 使用muke这个database */

drop table if exists t1;  /* 如果表t1存在则删除表t1 */

CREATE TABLE `t1` (	      /* 创建表t1 */
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `a` varchar(20) DEFAULT NULL,
  `b` int(20) DEFAULT NULL,
  `c` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB CHARSET=utf8mb4 ;

drop procedure if exists insert_t1; /* 如果存在存储过程insert_t1，则删除 */
delimiter ;;
create procedure insert_t1()        /* 创建存储过程insert_t1 */
begin
  declare i int;                  /* 声明变量i */
  set i=1;                        /* 设置i的初始值为1 */
  while(i<=10000)do			      /* 对满足i<=1000的值进行while循环 */
    insert into t1(a,b) values(i,i); /* 写入表t1中a、b两个字段，值都为i当前的值 */
    set i=i+1;                       /* 将i加1 */
  end while;
end;;
delimiter ;
call insert_t1();               /* 运行存储过程insert_t1 */
```
### 导出一条 SQL 包含多行数据的数据文件
为了获取批量导入数据的 SQL，首先对测试表的数据进行备份，备份的 SQL 为一条 SQL 包含多行数据的形式（执行环境为 Centos7 命令行）。

````sql
mysqldump -utest_user3 -p'userBcdQ19Ic' -h127.0.0.1 --set-gtid-purged=off --single-transaction --skip-add-locks  muke t1 >t1.sql
````
这里对上面 mysqldump 所使用到的一些参数做下解释：

| 参数 | 注释 |
| :---: | :---: |
| -utest_user3 | 用户名，这里使用的是root用户 |
| -p’userBcdQ19Ic’ | 密码 |
| -h127.0.0.1 | 连接的MySQL服务端IP |
| set-gtid-purged=off | 不添加SET @@GLOBAL.GTID_PURGED |
| –single-transaction | 设置事务的隔离级别为可重复读，即REPEATABLE READ，这样能保证在一个事务中所有相同的查询读取到同样的数据 |
| –skip-add-locks | 取消每个表导出之前加lock tables操作 |
| muke | 库名 |
| t1 | 表名 |
| t1.sql | 导出数据到这个文件 |

查看文件 t1.sql 内容，可看到数据是单条 SQL 有多条数据，如下：
```sql
DROP TABLE IF EXISTS `t1`;
/* 按照上面的备份语句备份的数据文件包含drop命令时，需要特别小心，在后续使用备份文件做导入操作时，应该确定所有表名，防止drop掉业务正在使用
......
CREATE TABLE `t1`......
......
INSERT INTO `t1` VALUES (1,'1',1,'2019-05-24 15:44:10'),(2,'2',2,'2019-05-24 15:44:10'),(3,'3',3,'2019-05-24 15:44:10')...
```
### 导出一条SQL只包含一行数据的数据文件
```sql
mysqldump -utest_user3 -p'userBcdQ19Ic' -h127.0.0.1 --set-gtid-purged=off --single-transaction --skip-add-locks --skip-extended-insert muke t1 >t1_row.sql
```
mysqldump命令参数解释：

| 参数 | 注释 |
| :---: | :---: |
| –skip-extended-insert | 一条SQL一行数据的形式导出数据 |

备份文件t1_row.sql内容如下:
```sql
INSERT INTO `t1` VALUES (1,'1',1,'2019-05-24 15:44:10');
INSERT INTO `t1` VALUES (2,'2',2,'2019-05-24 15:44:10');
INSERT INTO `t1` VALUES (3,'3',3,'2019-05-24 15:44:10');
...
```
### 导入时间的对比
首先导入一行 SQL 包含多行数据的数据文件：
```sql
root@mysqltest ~]# time mysql -utest_user3 -p'userBcdQ19Ic' -h127.0.0.1 muke <t1.sql

real	0m0.230s
user	0m0.007s
sys		0m0.003s
```
耗时0.2秒左右。

导入一条SQL只包含一行数据的数据文件：
```sql
[root@mysqltest ~]# time mysql -utest_user3 -p'userBcdQ19Ic' -h127.0.0.1 muke <t1_row.sql

real	0m31.138s
user	0m0.088s
sys		0m0.126s
```
耗时31秒左右。
### 结论
一次插入多行（一次插入）花费时间0.2秒，一次插入一行（多次插入）花费了31秒，对比效果明显，因此建议有大批量导入时，推荐一条insert语句插入多行数据。

## 关闭自动提交
### 对比开启和关闭自动提交的效率
auto commit 开启时会为每个插入执行提交。可以在InnoDB导入数据时，关闭自动提交。如下：
```sql
SET autocommit=0;
INSERT INTO `t1` VALUES (1,'1',1,'2019-05-24 15:44:10');
INSERT INTO `t1` VALUES (2,'2',2,'2019-05-24 15:44:10');
INSERT INTO `t1` VALUES (3,'3',3,'2019-05-24 15:44:10');
......
COMMIT;
```
### 环境准备
使用上方生成的t1_row.sql，

在insert前增加：SET autocommit=0;

在insert语句后面增加：COMMIT;

### 测试
```sql
[root@mysqltest muke]# time mysql -utest_user3 -p'userBcdQ19Ic' -h127.0.0.1 muke <t1_row.sql

real		 0m1.036s
user		0m0.062s
sys		   0m0.108s
```
开启自动提交的情况下导入是31秒。关闭自动提交的情况下导入是1秒左右，因此导入多条数据时，关闭自动提交，让多条 insert 一次提交，可以大大提升导入速度。

### 原因分析
与本节前面讲的一次插入多行能提高批量插入速度的原因一样，因为批量导入大部分时间耗费在客户端与服务端通信的时间，所以多条 insert 语句合并提交可以减少客户端与服务端通信的时间，并且合并提交还可以减少数据落盘的次数。

## 参数调整
影响MySQL写入速度的主要两个参数：innodb_flush_log_at_trx_commit、sync_binlog。

### 参数解释
innodb_flush_log_at_trx_commit：控制重做日志刷新到磁盘的策略，有0 、1和2三种值。

*  0：master线程每秒把redo log buffer写到操作系统缓存，再刷到磁盘；

*  1：每次提交事务都将redo log buffer写到操作系统缓存，再刷到磁盘；

*  2：每次事务提交都将redo log buffer写到操作系统缓存，由操作系统来管理刷盘。

        具体原理会在后续的事务这章进行详细描述。
 
 sync_binlog：控制binlog的刷盘时机，可配置0、1或者大于1的数字。


*  0：二进制日志从不同步到磁盘，依赖OS刷盘机制；
*  1：二进制日志每次提交都会刷盘；
*  n(n>1) : 每n次提交落盘一次。  

      innodb_flush_log_at_trx_commit设置为0、同时sync_binlog设置为0时，
      写入数据的速度是最快的。如果对数据库安全性要求不高(比如你的测试环境)，
      可以尝试都设置为0后再导入数据，能大大提升导入速度。

## 总结
今天一起研究了怎样提高 MySQL 批量导入数据的速度。根据测试，总结了加快批量数据导入有如下方法：


* 一次插入多行的值；
* 关闭自动提交，多次插入数据的 SQL 一次提交；
* 调整参数，innodb_flush_log_at_trx_commit 和 sync_binlog 都设置为0（当然这种情况可能会丢数据）







